from __future__ import annotations

from typing import Dict


# Minimal Persian rule-based sentiment analyzer with emoji support
# This is intentionally simple and fast with small lexicons.

POSITIVE_WORDS = {
    "Ø¹Ø§Ù„ÛŒ",
    "Ø®ÙˆØ¨",
    "Ø²ÛŒØ¨Ø§",
    "Ø¯ÙˆØ³Øªâ€ŒØ¯Ø§Ø´ØªÙ†ÛŒ",
    "Ø¯ÙˆØ³ØªØ¯Ø§Ø´ØªÙ†ÛŒ",
    "Ø´Ø§Ø¯",
    "Ø§Ù…ÛŒØ¯ÙˆØ§Ø±",
    "Ø§Ù…ÛŒØ¯Ø¨Ø®Ø´",
    "Ù…ÙˆÙÙ‚",
    "Ù…ÙˆÙÙ‚ÛŒØª",
    "Ø±Ø§Ø¶ÛŒ",
    "Ù…Ø«Ø¨Øª",
    "Ø±Ø´Ø¯",
    "Ø§ÙØ²Ø§ÛŒØ´",
    "Ø¨Ù‡Ø¨ÙˆØ¯",
    "ØµØ¹ÙˆØ¯",
    "ØµØ¹ÙˆØ¯ÛŒ",
    "Ø®ÙˆØ´Ø¨ÛŒÙ†",
    "Ø®ÙˆØ´â€ŒØ¨ÛŒÙ†",
    "Ø®ÙˆØ´Ø¨ÛŒÙ†Ø§Ù†Ù‡",
}

NEGATIVE_WORDS = {
    "Ø¨Ø¯",
    "Ø§ÙØªØ¶Ø§Ø­",
    "Ø²Ø´Øª",
    "Ù†ÙØ±Øª",
    "Ù…ØªÙ†ÙØ±",
    "ØºÙ…Ú¯ÛŒÙ†",
    "Ù†Ø§Ø±Ø§Ø­Øª",
    "Ù†Ø§Ú©Ø§Ù…",
    "Ø¹ØµØ¨Ø§Ù†ÛŒ",
    "Ù…Ù†ÙÛŒ",
    "Ú©Ø§Ù‡Ø´",
    "Ø§ÙØª",
    "Ø³Ù‚ÙˆØ·",
    "Ø±ÛŒØ²Ø´",
    "Ø¶Ø¹ÛŒÙ",
    "Ù†Ø§Ø§Ù…ÛŒØ¯",
    "Ø¯Ù„ÙˆØ§Ù¾Ø³",
    "Ù†Ú¯Ø±Ø§Ù†",
}

EMOJI_TO_SCORE = {
    "ðŸ‘": 1.0,
    "ðŸ˜Š": 1.0,
    "ðŸ˜": 1.0,
    "ðŸ˜": 1.0,
    "â¤ï¸": 1.0,
    "ðŸŽ‰": 1.0,
    "ðŸ‘Ž": -1.0,
    "ðŸ˜ž": -1.0,
    "ðŸ˜¡": -1.0,
    "ðŸ’”": -1.0,
    "ðŸ˜¢": -1.0,
}


def _simple_tokenize(text: str) -> list[str]:
    separators = "\n\t .,!?:;ØŒØ›()[]{}\"'Â«Â»\/|"
    tokens: list[str] = []
    current = []
    for ch in text:
        if ch in separators:
            if current:
                tokens.append("".join(current))
                current = []
            # keep emojis and standalone punctuation as separate tokens
            if ch.strip() and ch in EMOJI_TO_SCORE:
                tokens.append(ch)
        else:
            # keep emojis as their own tokens
            if ch in EMOJI_TO_SCORE:
                if current:
                    tokens.append("".join(current))
                    current = []
                tokens.append(ch)
            else:
                current.append(ch)
    if current:
        tokens.append("".join(current))
    return tokens


def analyze_text(text: str) -> Dict[str, object]:
    if not text:
        return {"label": "neutral", "scores": {"positive": 0.0, "negative": 0.0, "neutral": 1.0}}

    tokens = _simple_tokenize(text)

    positive_hits = 0
    negative_hits = 0

    for tok in tokens:
        normalized = tok.strip().lower()
        if not normalized:
            continue
        if tok in EMOJI_TO_SCORE:
            if EMOJI_TO_SCORE[tok] > 0:
                positive_hits += 1
            elif EMOJI_TO_SCORE[tok] < 0:
                negative_hits += 1
            continue
        if normalized in (w.lower() for w in POSITIVE_WORDS):
            positive_hits += 1
            continue
        if normalized in (w.lower() for w in NEGATIVE_WORDS):
            negative_hits += 1

    # Heuristic keyword scanning for common market phrases (Persian)
    lowered = text.strip().lower()
    positive_keywords = [
        "Ø±Ø´Ø¯", "Ø§ÙØ²Ø§ÛŒØ´", "ØµØ¹ÙˆØ¯", "Ø®ÙˆØ´Ø¨ÛŒÙ†", "Ø§Ù…ÛŒØ¯", "Ø§Ù…ÛŒØ¯Ø¨Ø®Ø´", "Ø¨Ù‡Ø¨ÙˆØ¯", "Ù…Ø«Ø¨Øª", "Ø±ÙˆÙ†Ù‚", "Ø¨Ø§Ø²Ú¯Ø´Øª",
    ]
    negative_keywords = [
        "Ú©Ø§Ù‡Ø´", "Ø§ÙØª", "Ø³Ù‚ÙˆØ·", "Ø±ÛŒØ²Ø´", "Ù†Ú¯Ø±Ø§Ù†", "ØªØ±Ø³", "Ø¶Ø¹ÛŒÙ", "Ù…Ù†ÙÛŒ", "Ø±Ú©ÙˆØ¯", "Ø®Ø±ÙˆØ¬ Ø³Ø±Ù…Ø§ÛŒÙ‡",
    ]
    for kw in positive_keywords:
        if kw in lowered:
            positive_hits += 1
    for kw in negative_keywords:
        if kw in lowered:
            negative_hits += 1

    total_hits = positive_hits + negative_hits
    if total_hits == 0:
        scores = {"positive": 0.0, "negative": 0.0, "neutral": 1.0}
        label = "neutral"
    else:
        pos_score = positive_hits / total_hits
        neg_score = negative_hits / total_hits
        neu_score = max(0.0, 1.0 - (pos_score + neg_score) / 1.0)
        scores = {"positive": round(pos_score, 4), "negative": round(neg_score, 4), "neutral": round(neu_score, 4)}
        if abs(pos_score - neg_score) < 0.15:
            label = "neutral"
        elif pos_score > neg_score:
            label = "positive"
        else:
            label = "negative"

    return {"label": label, "scores": scores}


